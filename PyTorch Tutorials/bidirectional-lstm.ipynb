{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-27T18:44:23.988815Z","iopub.execute_input":"2024-05-27T18:44:23.989204Z","iopub.status.idle":"2024-05-27T18:44:24.459008Z","shell.execute_reply.started":"2024-05-27T18:44:23.989173Z","shell.execute_reply":"2024-05-27T18:44:24.457556Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\nimport torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\nimport torch.nn.functional as F  # All functions that don't have any parameters\nfrom torch.utils.data import (\n    DataLoader,\n)  # Gives easier dataset managment and creates mini batches\nimport torchvision.datasets as datasets  # Has standard datasets we can import in a nice way\nimport torchvision.transforms as transforms  # Transformations we can perform on our dataset\nfrom tqdm import tqdm  # progress bar","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:51:02.238174Z","iopub.execute_input":"2024-05-27T18:51:02.238596Z","iopub.status.idle":"2024-05-27T18:51:02.247575Z","shell.execute_reply.started":"2024-05-27T18:51:02.238564Z","shell.execute_reply":"2024-05-27T18:51:02.245941Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"device= \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:51:06.469431Z","iopub.execute_input":"2024-05-27T18:51:06.469849Z","iopub.status.idle":"2024-05-27T18:51:06.475912Z","shell.execute_reply.started":"2024-05-27T18:51:06.469818Z","shell.execute_reply":"2024-05-27T18:51:06.474851Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Hyperparatemers\n\ninput_size= 28\nhidden_size= 256\nnum_layers= 2\nsequence_length= 28\nnum_classes= 10\nlearning_rate= 0.005\nbatch_size= 64\nnum_epochs= 3","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:51:14.106417Z","iopub.execute_input":"2024-05-27T18:51:14.106868Z","iopub.status.idle":"2024-05-27T18:51:14.113785Z","shell.execute_reply.started":"2024-05-27T18:51:14.106832Z","shell.execute_reply":"2024-05-27T18:51:14.112307Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class BRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(BRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(\n            input_size, hidden_size, num_layers, batch_first=True, bidirectional=True\n        )\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n\n        out, _ = self.lstm(x)\n        out = self.fc(out[:, -1, :])\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:51:20.251936Z","iopub.execute_input":"2024-05-27T18:51:20.252320Z","iopub.status.idle":"2024-05-27T18:51:20.263200Z","shell.execute_reply.started":"2024-05-27T18:51:20.252292Z","shell.execute_reply":"2024-05-27T18:51:20.261892Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train_dataset = datasets.MNIST(\n    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n)\n\ntest_dataset = datasets.MNIST(\n    root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True\n)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:51:38.514098Z","iopub.execute_input":"2024-05-27T18:51:38.514527Z","iopub.status.idle":"2024-05-27T18:51:38.623992Z","shell.execute_reply.started":"2024-05-27T18:51:38.514492Z","shell.execute_reply":"2024-05-27T18:51:38.622621Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model = BRNN(input_size, hidden_size, num_layers, num_classes).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:51:48.061969Z","iopub.execute_input":"2024-05-27T18:51:48.062339Z","iopub.status.idle":"2024-05-27T18:51:48.090106Z","shell.execute_reply.started":"2024-05-27T18:51:48.062311Z","shell.execute_reply":"2024-05-27T18:51:48.088987Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:51:58.753707Z","iopub.execute_input":"2024-05-27T18:51:58.754089Z","iopub.status.idle":"2024-05-27T18:51:58.759812Z","shell.execute_reply.started":"2024-05-27T18:51:58.754061Z","shell.execute_reply":"2024-05-27T18:51:58.758728Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n        # Get data to cuda if possible\n        data = data.to(device=device).squeeze(1)\n        targets = targets.to(device=device)\n\n        # forward\n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:52:08.276745Z","iopub.execute_input":"2024-05-27T18:52:08.277168Z","iopub.status.idle":"2024-05-27T19:10:00.621005Z","shell.execute_reply.started":"2024-05-27T18:52:08.277134Z","shell.execute_reply":"2024-05-27T19:10:00.619456Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"100%|██████████| 938/938 [06:03<00:00,  2.58it/s]\n100%|██████████| 938/938 [05:59<00:00,  2.61it/s]\n100%|██████████| 938/938 [05:49<00:00,  2.68it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"def check_accuracy(loader, model):\n    if loader.dataset.train:\n        print(\"Checking accuracy on training data\")\n    else:\n        print(\"Checking accuracy on test data\")\n\n    num_correct = 0\n    num_samples = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n        print(\n            f\"Got {num_correct} / {num_samples} with accuracy  \\\n              {float(num_correct)/float(num_samples)*100:.2f}\"\n        )\n\n    model.train()\n\n\ncheck_accuracy(train_loader, model)\ncheck_accuracy(test_loader, model)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:10:00.623958Z","iopub.execute_input":"2024-05-27T19:10:00.624486Z","iopub.status.idle":"2024-05-27T19:11:59.766518Z","shell.execute_reply.started":"2024-05-27T19:10:00.624437Z","shell.execute_reply":"2024-05-27T19:11:59.764944Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Checking accuracy on training data\nGot 58977 / 60000 with accuracy                98.30\nChecking accuracy on test data\nGot 9794 / 10000 with accuracy                97.94\n","output_type":"stream"}]}]}